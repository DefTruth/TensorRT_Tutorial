最近一直在做 Conformer 模型的 TensorRT 加速，总结了几种常用的 TensorRT 转换模型方法，分别是转换工具，TensorRT API 搭建和转换 和超大Plugin。根据自己的见解，对这三种方法进行分析和比较。

## 一、 介绍
### 1.1 转换工具
转换工具可以分为三小类：（1）训练框架自带 TRT 工具；（2）第三方转换工具；（3）TVM。
框架框架自带TRT工具，主要代表是TF-TRT和Torch-TensorRT。顾名思义，这两个工具分别是针对TensorFlow和PyTorch模型，由官方支持，将相对应的模型转成TensorRT格式。
第三方 转换工具 的典型代表就是 onnx-tensorrt parser，原先应该是个人维护，后来NVIDIA接手了。
TVM个人感觉不太成熟，尤其是在支持动态输入方面，了解的也不多，这里不多评价。

**开发流程**
以 onnx-tensorrt parser 为例。对于标准模型非常简单，几行代码就能搞定。
对于需要使用plugin 情况，就比较麻烦了。比如layernorm算子加速，开发流程为：（1）编写 layernorm plugin 和测试代码；（2）编写onnx 算子替换代码，从模型结构中搜索出layernorm散装算子，进行替换，生成新onnx模型；（3）编写转换代码，转换模型。

举例代码：https://github.com/NVIDIA/trt-samples-for-hackathon-cn/tree/master/Hackathon/2022/code/LayerNormPlugin

### 1.2 TensorRT API 搭建和转换
转换工具一般是黑盒，输入模型，直接就输出了转换完 TensorRT 模型，非常方便。
使用 TensorRT API 包含搭建网络和转换模型两个步骤，其中还包含各种配置参数，步骤麻烦。加上TensorRT API 设计真的一言难尽，使用 API 转换网络属实费时费力。正因为如此，才催生了各种转换工具。
但 API 也是有优点的，在某些场景下，使用API方式构建更方便，甚至是必须的。比如（1）使用现有TRT转换工具无法成功转换，存在不支持的算子，比如fastmoe结构；（2）想要对模型做深度合并加速，比如TensorRT OOS中的BERT demo。

**开发流程**
1. 熟悉整个网络结构，很细节的那种。需要看透模型训练代码或者给的onnx模型。
2. 如果是只能拿到onnx模型，则需要分析读取模型权值。直接拿到训练代码的可以省去这步。
3. 调用TensorRT API，对模型中的算子进行一一替换，填充TensorRT Network。
3. 编写每一个 plugin 和单元测试代码，并嵌入到网络中。
4. 设置构建参数，调用构建API 构建网络。

举例代码：https://github.com/wang-xinyu/tensorrtx/tree/master/alexnet
这个git里用的是c++ api，构建网络推荐使用python api，更简单方便。

### 1.3 超大 Plugin
超大 Plugin 是指将整个模型或者某个大模块实现在一个Plugin内，在plugin内部非常自由。一套代码基本只适用于一类模型，开发成本较高。目前只见过 NV 的团队针对一类模型做这样的事情，比如 FasterTransformer。

**开发流程**
1. 熟悉整个网络结构，很细节的那种。需要看透模型训练代码或者给的onnx模型。
2. 分析读取模型权值。
3. 调用库 or 写 kernel 实现所有算子；应该写一套测试代码即可。
4. 填充网络，设置构建参数，调用构建API 构建网络。
PS：这种方法一般没几个layer，填充网络步骤比较简单，就和构建网络合并在一起了。

举例代码就是FasterTransformer了。

## 二、 分析和比较
上述三种转换方式，在实际应用中，从易用性，开发成本，性能，和灵活性四个方面进行分析，数字代表星级，最高5星：

| 方法       | 易用性 | 开发成本 | 性能 | 灵活性 |
| ---        | ---    | ---      | ---  | ---    |
| 转换工具   | 4      | 2        | 2    |  2
| API        | 1      | 5        | 5    |  5
| 超大Plugin | 1      | 3.5        | 5    |  4


**易用性**
易用性这里指使用难度。毫无疑问，转换工具就是为了解决TensorRT 使用难度高痛点而生的。各大转换工具基本都可以零门槛，几行代码实现转换模型。
但遇见需要使用plugin情况会麻烦一些，评级4星。
API和超大Plugin也是毫无疑问，门槛比较高。前者需要熟练TensorRT的API和构建步骤，后者需要熟练CUDA。评星最低1星。

**开发成本**
开发成本和易用性其实比较像，只所以单独拎出来比较一下，是因为API和超大Plugin虽然开发成本都很高，但也有些许差别。
从开发流程可以看出，API方法多了填充网络（这个是大头）和plugin单元测试代码的工作量，开发成本更高一些。

**性能**
转换工具的最大缺点就是性能。转换工具的本质就是使用TensorRT API，将网络结构一对一的转换成TensorRT模型，是强依赖于模型结构的。而受python语言和模型训练限制，模型结构往往离最优还有不少的距离。但TensorRT 8.4开始，着重增强了mylein的融合算子功能，就算模型结构不是最优，trt本身也能在构建模型的过程中尽可能的提升性能，所以评星2星。
API和超大Plugin难度高，开发成本还高的情况下，还一直有不少人使用，就算可以追求极致速度，尤其在目前“降本增效”的大环境下，速度的提升显得更重要一些。两者都可以实现大部分优化策略，存在一些细微的差别。
1. 在int8加速方面，API方法不受影响。但超大Plugin 实现 int8就麻烦太多了。但目前的显卡fp16的性能就已经很不错了，int8的性能提升也就没有之前那么诱人了。
2. TensorRT 的加速技术之一就是可以适配硬件和输入大小选择运行最快的实现方式。超大Plugin也基本跟这个无缘了，但对于gemm等算子，也可以通过cublas/lt的方式选择最优的kernel。
3. 超大Plugin可以实现稀疏矩阵乘、INT4矩阵乘等新技术，API方式实现这些就只能等TensorRT的更新了。

**灵活性**
实际产业应用中，经常有魔改模型和加速模型等需求，灵活性指的就是是否能够满足这类需求。
转换工具满足魔改模型需求还行，加速模型方面实在是比较麻烦，评星2星。
API方法在灵活性方面是最高的，能够轻松满足各种需求，评星5星。
超大Plugin方式，一套代码基本只适用于一类模型，每次魔改模型可能发生二次开发和加配置参数这种情况，评星4星。

## 三、个人的一些看法
个人认为，在NLP和语音领域，API方法是最适用于工业领域的TensorRT上线方法。尤其是在目前降本增效的大环境下，有能力的话，直接上API。（最适用并不代表一定要用……）
为什么 NLP 和 语音领域？因为这两个领域的模型结构相对比较复杂，输入大小一般是动态的，模型结构还属于不断更新的状态（主要还是看google……），向大模型发展等原因，使用现成的转换工具，往往速度不够好。
